{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import asarray, insert, ndarray, piecewise\n",
    "class Perceptron(ndarray):\n",
    "    \n",
    "    def __new__(cls, weights):\n",
    "        return asarray(weights).view(cls)\n",
    "    \n",
    "    def activate(self,inputs):\n",
    "        try:\n",
    "            assert(len(inputs)+1==len(self))\n",
    "        except:\n",
    "            return 'input vector of dimension {} is not valid for a bias/weight vector of dimension {}'.format(\n",
    "                len(inputs), len(self))\n",
    "        inputs = insert(inputs,0,-1)\n",
    "        return self._threshold(self.dot(inputs))\n",
    "\n",
    "    def update(self,inputs,target,eta=.1):\n",
    "\n",
    "        result = self.activate(inputs)\n",
    "        inputs = insert(inputs,0,-1)\n",
    "        self[1:] += eta*(target-result)*inputs[1:]\n",
    "          \n",
    "    def _threshold(self, energy):\n",
    "            \n",
    "        return piecewise(energy, [energy < 0, energy >= 0], [0 , 1])  \n",
    "                           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_1 = Perceptron([0,-1,.2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3583900315119233e-09"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_1.activate([3,3,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.099750489119685135"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_1.activate([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.44828551 -3.44828551 -2.24828551  0.55171449]\n",
      "[ 2.49415566 -3.49415566 -2.29415566  0.50584434]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99958074949915265"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p_1)\n",
    "for _ in range(1000):\n",
    "    p_1.update([1,1,1],1)\n",
    "print(p_1)\n",
    "p_1.activate([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "requests.post?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Meditation\n",
    "What do you think the advantage of a **perceptron** is, compared with simply returning the dot product without a threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to train Perceptrons\n",
    "We want to build networks of **perceptrons** that contain interesting functions. What are the parameters of the perceptrons that we will want to modify?\n",
    "\n",
    "1. Output functions\n",
    "1. Input Values\n",
    "1. Thresholds\n",
    "1. Weights\n",
    "1. Input functions\n",
    "\n",
    "<!-- 3, 4 Sure! We can modify the thresholds, or we could also include threshold changes in the weights. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Inputs\n",
    "**Neural networks** are built out of components like perceptron units. What do inputs to networks of perceptrons look like?\n",
    "\n",
    "1. A matrix of numerical, values, with some labeled input\n",
    "1. A directed graph.\n",
    "1. An unlabeled matrix of numerical values.\n",
    "1. A set of classifications of numerical values.\n",
    "1. A matrix of numerical values with classifications for each row.\n",
    "\n",
    "<!-- 5 A single perceptron is very much like linear regression. Therefore it should take the same kinds of inputs. However the outputs of perceptrons will generally be classifications, not numerical. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Inputs\n",
    "What information can we get as the output of a **neural network**?\n",
    "\n",
    "1. A directed graph, the neural network itself.\n",
    "1. A single scalar-valued number.\n",
    "1. The classification of a vector.\n",
    "1. A vector valued output for any vector input.\n",
    "\n",
    "<!-- All. In general, neural nets are much more flexible than thresholded perceptron networks! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Update Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "        \n",
    "    def __init__(self,weights,threshold):\n",
    "        self.weights = np.array(weights)\n",
    "        self.threshold = np.array(threshold)\n",
    "    \n",
    "    def activate(self,inputs):\n",
    "        '''Takes in @param inputs, a list of numbers.\n",
    "        @return the output of a threshold perceptron with\n",
    "        given weights, threshold, and inputs.\n",
    "        ''' \n",
    "        \n",
    "        activation = np.dot(self.weights,inputs)\n",
    "        return self._threshold(activation)\n",
    "        \n",
    "\n",
    "    def update(self,values,train,eta=.1):\n",
    "        '''Takes in a 2D array @param values consisting of a LIST of inputs\n",
    "        and a 1D array @param train, consisting of a corresponding list of \n",
    "        expected outputs.\n",
    "        Updates internal weights according to the perceptron training rule\n",
    "        using these values and an optional learning rate, @param eta.\n",
    "        '''\n",
    "        for target,value in zip(targets,values):\n",
    "            result = self.activate(value)\n",
    "            self.weights = self.weights + eta*(target-result)*value\n",
    "               \n",
    "        \n",
    "    def _threshold(self, energy):\n",
    "            \n",
    "        return np.piecewise(energy, [energy < self.threshold, energy >= self.threshold], [0 , 1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_1 = Perceptron([-1,.2,3],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "values = [np.array([3,1,2]),np.array([2,-1,4]),np.array([0,-5,10]),np.array([1,1,2])]\n",
    "targets = np.array([0,0,1,1])\n",
    "for value in values:\n",
    "    print(p_1.activate(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "p_1.update(values,targets)\n",
    "p_1.weights\n",
    "for value in values:\n",
    "    print(p_1.activate(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "p_1.update(values,targets)\n",
    "p_1.weights\n",
    "for value in values:\n",
    "    print(p_1.activate(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "p_1.update(values,targets)\n",
    "p_1.weights\n",
    "for value in values:\n",
    "    print(p_1.activate(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "p_1.update(values,targets)\n",
    "p_1.weights\n",
    "for value in values:\n",
    "    print(p_1.activate(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layered Network Update\n",
    "In general we place units together to form **layered networks**. This will be represented as follows:\n",
    "\n",
    "     [[node, node, node],   # input layer\n",
    "     [node, node],          # hidden layer\n",
    "     [node]]                # output layer\n",
    "     \n",
    "Given weights for the hidden layer of `[1, 1, -5]` and `[3, -4, 2]`, and weights for the output layer of `[2, -1]`, what will this network output for inputs `[1, 2, 3]`?\n",
    "\n",
    "<!-- -25 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Representational Power\n",
    "We would like it if these additional layers gave us more representational power. For linear units, just taking weighted sums, they will not. Given the following network, where each node simply passes along the dot product of its inputs with its weights, write down the weights of a single linear node that computes the same function.\n",
    "\n",
    "    [[input_1, input_2],\n",
    "    [[3, 2], [-1, 4], [3, -5]],\n",
    "    [[1, 2, -1]]]\n",
    "    \n",
    "    input_1 = \n",
    "    input_2 = \n",
    "    \n",
    "<!-- -2, 15 -->    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the XOR Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#   In this exercise, you will create a network of perceptrons which\n",
    "#   represent the xor function use the same network structure you used\n",
    "#   in the previous quizzes.\n",
    "#\n",
    "#   You will need to do two things:\n",
    "#   First, create a network of perceptrons with the correct weights\n",
    "#   Second, define a procedure EvalNet() which takes in a list of \n",
    "#   inputs and ouputs the value of this network.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "\n",
    "    def evaluate(self,values):\n",
    "        '''Takes in @param values, @param weights lists of numbers\n",
    "        and @param threshold a single number.\n",
    "        @return the output of a threshold perceptron with\n",
    "        given weights and threshold, given values as inputs.\n",
    "        ''' \n",
    "               \n",
    "        #First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values[i],self.weights[i])\n",
    "        \n",
    "        #Then evaluate the return value of the perceptron\n",
    "        if strength >= self.threshold:\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __init__(self,weights=None,threshold=None):\n",
    "        if weights is not None:\n",
    "            self.weights = weights\n",
    "        if threshold is not None:\n",
    "            self.threshold = threshold\n",
    "            \n",
    "\n",
    "Network = [\n",
    "    #input layer, declare perceptrons here\n",
    "    [ ... ], \\\n",
    "    #output node, declare one perceptron here\n",
    "    [ ... ]\n",
    "]\n",
    "\n",
    "\n",
    "def EvalNetwork(inputValues, Network):\n",
    "    \n",
    "    \n",
    "    # Be sure your output values are single numbers\n",
    "    return OutputValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretion Quiz\n",
    "One problem that perceptron units have is that their outpus are discrete. This makes it difficult to address regression problems with them, and requires them to have more complexity to capture some concepts. \n",
    "\n",
    "For example:\n",
    "\n",
    "Given a network of perceptrons with struction `[2, 2, 1]`, that is, two input nodes, two hidden nodes, and one output node, how many possible prices could the network assign to a house?\n",
    "\n",
    "<!-- 4 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuity\n",
    "As discussed in the previous lesson, to solve the problem of having only a very few discrete outputs from our neural net, we'll apply a transition function.\n",
    "\n",
    "We'll start by letting you test out a variety of functions, numerically approximating their derivatives in order to apply a gradient descent update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#   Python Neural Networks code originally by Szabo Roland and used by permission\n",
    "#\n",
    "#   Modifications, comments, and exercise breakdowns by Mitchell Owen, (c) Udacity\n",
    "#\n",
    "#   Retrieved originally from http://rolisz.ro/2013/04/18/neural-networks-in-python/\n",
    "#\n",
    "#\n",
    "#\tNeural Network Sandbox\n",
    "#\n",
    "#\tDefine an activation function activate(), which takes in a number and returns a number.\n",
    "#\tUsing test run you can see the performance of a neural network running with that activation function.\n",
    "#\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def activate(strength):\n",
    "    return np.power(strength,2)\n",
    "    \n",
    "def activation_derivative(activate, strength):\n",
    "    #numerically approximate\n",
    "    return (activate(strength+1e-5)-activate(strength-1e-5))/(2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "We have decided that we need a function which is continous (to avoid the discrete problem of perceptrons) but not linear (to allow us to represent non-linear functions). Which of the following seems appropriate?\n",
    "\n",
    "1. Sine\n",
    "1. Arctangent\n",
    "1. Natural logarithm\n",
    "1. Cube root\n",
    "1. Logistic Function\n",
    "\n",
    "<!-- 5 Great choice! Computing the derivative is essentially the same as computing the function itself, so this is also a relatively efficient choice. --> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Vs Sigmoid\n",
    "What will the difference be between a single perceptron and a sigmoid unit on binary classification problems?\n",
    "\n",
    "1. There will be no difference\n",
    "1. One gives more information but they will give the same answer\n",
    "1. They will sometimes give different answers.\n",
    "1. They will always give different answers.\n",
    "1. They will give different answers in certain rare circumstances.\n",
    "\n",
    "<!-- 2 Great choice! Computing the derivative is essentially the same as computing the function itself, so this is also a relatively efficient choice. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Learning\n",
    "We need to train our network of sigmoid units like we trained network of perceptrons. How should we determine our update rules?\n",
    "\n",
    "1. Arbitrarily\n",
    "1. Using our intuition\n",
    "1. Using domain knowledge\n",
    "1. Using calculus\n",
    "1. Using trigonometry\n",
    "\n",
    "<!-- 4 Right! We want to deal with small, gradual changes of continuous functions. This is exactly where we should use calculus!\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Issues\n",
    "Using calculus, **gradient descent** can provide us with a locally optimal set of weight changes, under certain assumptions. However, some issues can arise. Which of the following do you think could be problematic?\n",
    "\n",
    "1. local extrema\n",
    "1. lengthy run times\n",
    "1. infinite loops\n",
    "1. failure to completely converge\n",
    "\n",
    "<!-- 1,2, 4 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Programming Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#   As with the perceptron exercise, you will modify the\n",
    "#   last functions of this sigmoid unit class\n",
    "#\n",
    "#   There are two functions for you to finish:\n",
    "#   First, in activate(), write the sigmoid activation function\n",
    "#\n",
    "#   Second, in train(), write the gradient descent update rule\n",
    "#\n",
    "#   NOTE: the following exercises creating classes for functioning\n",
    "#   neural networks are HARD, and are not efficient implementations.\n",
    "#   Consider them an extra challenge, not a requirement!\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "        \n",
    "    def activate(self,values):\n",
    "        '''Takes in @param values, @param weights lists of numbers\n",
    "        and @param threshold a single number.\n",
    "        @return the output of a threshold perceptron with\n",
    "        given weights and threshold, given values as inputs.\n",
    "        ''' \n",
    "               \n",
    "        #First calculate the strength with which the perceptron fires\n",
    "        strength = self.strength(values)\n",
    "        self.last_input = strength\n",
    "        \n",
    "        #YOUR CODE HERE\n",
    "        #modify strength using the sigmoid activation function\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def strength(self,values):\n",
    "        strength = np.dot(values,self.weights)\n",
    "        return strength\n",
    "        \n",
    "    def update(self,values,train,eta=.1):\n",
    "        '''\n",
    "        Updates the sigmoid unit with expected return\n",
    "        values @param train and learning rate @param eta\n",
    "        \n",
    "        By modifying the weights according to the gradient descent rule\n",
    "        '''\n",
    "        \n",
    "        #YOUR CODE HERE\n",
    "        #modify the perceptron training rule to a gradient descent\n",
    "        #training rule you will need to use the derivative of the\n",
    "        #logistic function evaluated at the last input value.\n",
    "        #Recall: d/dx logistic(x) = logistic(x)*(1-logistic(x))\n",
    "        \n",
    "        result = self.activate(values)\n",
    "        for i in range(0,len(values)):\n",
    "            self.weights[i] += eta*(train - result)*values[i]\n",
    "        \n",
    "    def __init__(self,weights=None):\n",
    "        if weights:\n",
    "            self.weights = weights\n",
    "            \n",
    "            \n",
    "unit = Sigmoid(weights=[3,-2,1])\n",
    "unit.update([1,2,3],[0])\n",
    "print unit.weights\n",
    "#Expected: [2.99075, -2.0185, .97225]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
